\chapter{Analysis of a new type of time integration methods}

  \paragraph{}
  In this thesis, we were interested in finding solutions to steady problems.
  As we explained in the previous part, we use implicit time integration methods to get efficiently solutions of such problems.
  This is a pretty standard choice: most computational fluid dynamics solvers use implicit time integration methods to solve steady problems.
  The reason is because they can use larger time steps than their explicit counterparts.
  Because of this advantage, they are also often used when solving unsteady problems but with large time steps.
  Indeed, solving an unsteady problem with large time steps is similar to solving a steady problem.
  Most of what is done towards the steady problem solve can therefore be reused for unsteady problem solves with large time steps.

  \paragraph{}
  Even if implicit time integration methods are quite standard when solving problems with large time steps, there are other less conventional methods we could choose from.
  We could step out from the explicit implicit dichotomy, and decide to use IMplicit-EXplicit, or IMEX, methods.
  They split the function from the ordinary differential equation (\ref{eq:ode}) in two parts: the stiff part than is integrated by an implicit method, because of its stiffness, and the other part that can just be integrated with an explicit method.
  The Additive Semi-Implicit Runce--Kutta methods, or ASIRK methods, are such methods \cite{Zhong1996}.
  They are already in use in computational fluid dynamics problems, such as fluid-structure interaction problems \cite{HuangPerssonZahr2019}.
  Previous work already implemented ASIRK methods in our solver CEDRE for specific multiphysic applications.
  Some methods are even less common and correspond to a total paradigm shift: the parallel time integration methods \cite{Nievergelt1964, LionsMadayTurinici2001}.
  Just as we classically split the computational domain over processes and compute the spatial discretisation method in parallel, parallel time integration methods decompose the time integration interval into subintervals.
  They then solve the ordinary differential equation on each interval concurrently then ensure the continuity between the subintervals.
  They then iterate with Newton's method to find the solution over the whole time integration interval.
  As a result, they can approximate accurately the solution at later time without knowing accurately the solution at previous time.
  Despite being nontraditional, they were successfully used to solve fluid structure interaction problems or Navier--Stokes problems \cite{GanderVandewalle2007}.
  They were even more recently used to solve simple turbulent flow problems \cite{Lunet2018}.
  They can even be used in a more convoluted way, with for instance exponential methods \cite{GanderGuettel2013}.
  Just as parallel time integration is inspired from parallel spatial discretisation, some time integration methods are inspired from spectral discretisation methods.
  Time spectral methods, that were originally used for fluid dynamic time periodic problems \cite{GopinathJameson2005, GopinathJameson2006}, are now used on non-periodic problems \cite{EkiciDjeddiLiEtAl2020}.
  But as both time parallel integration methods and time spectral methods are considered modern methods and highly unconventional, they do not seem appropriate for an industrial solver such as ours.


  \section{Exponential integration methods}

    \paragraph{}
    We decided to look at an other class of time integration methods: exponential methods.
    Despite being known for a long time \cite{Pope1963}, they were not widely used in computational fluid dynamics, because of some difficulties that we will explain later, but still interested some \cite{EdwardsTuckermanFriesnerEtAl1994}.
    They started to come back in the literature \cite{HochbruckOstermann2005} and are now being use on applications similar to ours \cite{NieZhangZhao2006, BhattKhaliqWade2018}.

    \paragraph{}
    We start from an ordinary differential equation
    \begin{equation}\label{eq:ode_2}
      \frac{\mathrm{d} y}{\mathrm{d} t} = f\left(y\right) \ .
    \end{equation}
    This is the same as the ordinary differential equation (\ref{eq:ode}) from the previous part but with different notations.
    The main idea of exponential integration methods is to start from the ordinary differential equation (\ref{eq:ode_2}) and split the function $f$ in a linear part and a nonlinear part:
    \begin{equation}
      \frac{\mathrm{d} y}{\mathrm{d} t} = Ly + N\left(y\right) \ .
    \end{equation}
    This decomposition is sometimes natural for some particular equations, but in the more general case it is always possible: it consists in choosing a linear part $L$ then setting the nonlinear part $N\left(y\right) = f(y) - Ly$.
    There are then an infinite number of decomposition, but we will see later that some are more interesting.

    \paragraph{}
    To define the time integration step, we start from the current estimate of the solution $y_n$ that we assume exact : $y_n = y\left(t_n\right)$.
    We note $\Delta t = t_{n+1} - t_n$ as we work here with a fixed $n$.
    We can integrate equation (\ref{eq:ode_2}) using the variation of constants formula to get:
    \begin{equation}\label{eq:ode_int}
      y\left(t_{n+1}\right) = e^{\Delta t L} y_n + \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} N\left(y\left(t\right)\right) \mathrm{d}t \ .
    \end{equation}
    The exponential integration methods then approximate the integral to compute the next value $y_{n+1}$.
    What defines the method is how it approximates this integral.
    As we can see in equation (\ref{eq:ode_int}), the linear part is treated exactly and the nonlinear one is approximated.
    If there is no nonlinear part, with $N = 0$, then the solution $y_{n+1}$ is exact.
    If there is no linear part, with $L = 0$, then equation (\ref{eq:ode_int}) transforms into
    \begin{equation}\label{eq:ode_int_classic}
      y\left(t_{n+1}\right) = y_n + \int_{t_n}^{t_{n+1}} N\left(y\left(t\right)\right) \mathrm{d}t
    \end{equation}
    and the exponential integration method behaves like a standard time integration method.
    This is the advantage of exponential integration methods: at best they are exact methods, and at worse they are equivalent to traditional methods.

    \paragraph{}
    Similarly to classic methods, there are explicit \cite{BhattKhaliqWade2018} and implicit \cite{NieZhangZhao2006} exponential integration methods.
    It depends on whether it uses $y_{n+1}$ or not to compute the integral from equation (\ref{eq:ode_int}).

    \paragraph{}
    Before continuing, we need to define some functions that are going to be convenient later on.
    We consider the functions $\varphi_k$, defined by:
    \begin{equation}
      \left\{\begin{aligned}
        \varphi_0 &: z \mapsto e^z \\
        \forall k \in \mathbb{N}^*, \varphi_{k} &: z \mapsto \int_0^1 e^{\left(1 - \theta\right)z} \frac{\theta^{k-1}}{\left(k-1\right)!} \mathrm{d}\theta
      \end{aligned}\right.\ .
    \end{equation}
    We could also define them using the recurrence relation:
    \begin{equation}
      \left\{\begin{aligned}
        \varphi_0 &: z \mapsto e^z \\
        \forall k \in \mathbb{N}, \varphi_{k+1} &: z \mapsto \frac{\varphi_k\left(z\right) - \varphi_k\left(0\right)}{z} \ .
      \end{aligned}\right.
    \end{equation}
    Finally, we can also use their analytic formula:
    \begin{equation}
      \forall k \in \mathbb{N}, \varphi_{k} : z \mapsto \sum_{i = 0}^{+\infty} \frac{z^i}{\left(i + k\right)!} \ .
    \end{equation}
    This analytic formula ensure it is well defined for squared matrices.


  \section{Exponential Euler method}

    \section{Definition of the exponential Euler method}

      \paragraph{}
      To better understand exponential integration methods, we take the most basic one: the exponential Euler method.
      Just as the Euler method assumes that $N\left(y\right)$ is constant and equal to $N\left(y_n\right)$ in equation (\ref{eq:ode_int_classic}), its exponential counterpart makes the same assumption but in equation (\ref{eq:ode_int}).
      It then gives:
      \begin{equation}
        y_{n+1} = e^{\Delta t L} y_n + \Delta t \varphi_1\left(\Delta t L\right) N\left(y_n\right)
      \end{equation}
      using the $\varphi_1$ function defined above, and finally:
      \begin{equation}
        y_{n+1} = y_n + \Delta t \varphi_1\left(\Delta t L\right) f\left(y_n\right) \ .
      \end{equation}
      We note the difference with the corresponding standard Euler method, for which the $\varphi$ function is replaced with the identity function.
      If there is no linear part in the decomposition, with $L = 0$, then nothing is treated differently by the exponential method, and we recover the standard explicit Euler method.

      \paragraph{}
      In this method, we take the linearisation $L = f'\left(y_n\right)$.
      This choice feels natural and minimise the error of the method as is shown in the following.
      This choice is frequent among exponential integration methods, despite some methods using a fixed linearisation $L = f'\left(y_0\right)$.
      The difference is that a new Jacobian matrix is required at each iteration of the time integration method, but it is something we are used to deal with our traditional methods.
      It is worth noting that we might want to try the implicit equivalent of this method by taking $N\left(y\right) = N\left(y_{n+1}\right)$ in equation (\ref{eq:ode_int}).
      However, because we took $L = f'\left(y_n\right)$, $N'\left(y\right) = 0$ and after a linearisation the two variants are equivalent.


    \subsection{Analysis of the exponential Euler method}

      \paragraph{}
      From the definition on the method and equation (\ref{eq:ode_int}), we have that the error made after one step is:
      \begin{equation}
        \begin{aligned}
          y\left(t_{n+1}\right) - y_{n+1} &= \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} N\left(y\left(t\right)\right) \mathrm{d}t  - \Delta t \varphi_1\left(\Delta t L\right) N\left(y_n\right) \\
          &= \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} \left( N\left(y\left(t\right)\right) - N\left(y_n\right) \right)
        \end{aligned}
      \end{equation}
      We take the Taylor serie of the nonlinearity:
      \begin{equation}
        N\left(y\right) = \sum_{i = 0}^{+\infty} \frac{N^{\left(i\right)}\left(y_n\right)}{i!}\left(y - y_n\right)^i
      \end{equation}
      and in particular, using the fact that
      \begin{equation}
        \begin{aligned}
          y\left(t\right) &= y_n + y'\left(t_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right) \\
          & = y_n + f\left(y_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right)
        \end{aligned}
      \end{equation}
      the partial serie of order 2 is:
      \begin{equation}
        N\left(y\left(t\right)\right) = N\left(y_n\right) + N'\left(y_n\right)f\left(y_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right) \ .
      \end{equation}
      The error of the method is then:
      \begin{equation}
        \begin{aligned}
          y\left(t_{n+1}\right) - y_{n+1} &= \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} \left( N'\left(y_n\right)f\left(y_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right) \right) \mathrm{d}t \\
          &= \Delta t^2\varphi_2\left(\Delta t L\right) N'\left(y_n\right)f\left(y_n\right) + O\left(\Delta t^3\right) \ .
        \end{aligned}
      \end{equation}
      We now understand the decomposition chosen earlier: if $L = f'\left(y_n\right)$ and therefore $N'\left(y_n\right) = 0$ then $y_{n+1} - y\left(t_n{n+1}\right) = O\left(\Delta t^3\right)$ and the method is of order 2.
      This is a noticeable property of this method: despite being a single step method it has a second order of accuracy.

      \paragraph{}
      It is much harder to analyse the stability of this method, and more generally of exponential methods.
      Indeed, the stability analysis is done on the Dahlquist test equation (\ref{eq:dahlquist}).
      But as this equation is linear, exponential methods can solve them exactly no matter the time step size.
      We can still write the single step equation (\ref{eq:single_step}) $y_{n+1} = g\left(\Delta tJ\right)y_n$ with $g\left(z\right) = e^z$, and get the corresponding stability region which is the left half complex plane.
      We could then conclude that the method is A-stable.
      The issue with exponential methods is that the standard stability analysis is no longer pertinent.
      We tried to do a better stability analysis for simple exponential methods such as the exponential Euler method, but we did not succeed.
      Some work in the literature do define some stability notions for exponential methods \cite{DuZhu2004}, but usually take a linear $N$.
      It goes against the idea that all the linear part of $f$ is in $L$ and $N$ is purely nonlinear.
      We did not find a stability analysis that was satisfying to us.
      We will probably find such analysis, today of maybe in the future, in research from a more mathematical context than the one we are in, so we did not insist on finding such an analysis.


  \section{Exponential Runge--Kutta methods}

    \paragraph{}
    When we introduced explicit methods, we used more convoluted approximations than the explicit Euler methods for the integral in equation (\ref{eq:ode_int_classic}).
    This gave the Runge--Kutta methods.
    We can do the same for the integral in equation (\ref{eq:ode_int}) to get exponential Runge--Kutta methods.
    The difference with a standard Runge--Kutta method is that the quadrature coefficients are now functions of the matrix $L$:
    \begin{equation}
      \left\{\begin{aligned}
        y_{n+1} &= e^{\Delta tL} y_n + \Delta t \sum_{i = 1}^k b_i\left(\Delta t L\right) N\left(y_{n,i}\right) \\
        \textrm{with}\quad y_{n,i} &= e^{c_i \Delta t} y_n + \Delta t \sum_{j = 1}^{i-1} a_{ij}\left(\Delta t L\right) f\left(y_{n,j}\right)
      \end{aligned}\right. \ .
    \end{equation}
    The Butcher tableau that we used to represent the Runge--Kutta methods is also used to represent exponential Runge--Kutta methods.
    For instance, the Butcher tableau of the exponential Euler method, which is a special simple case of exponential Runge--Kutta methods, is shown in table \ref{tab:exp_rk_butcher}.

    \begin{table}
      \begin{tabular}{P{.15\textwidth}P{.3\textwidth}P{.4\textwidth}}
        \begin{tabular}{c|c}
          0 \RKBar $\varphi_1$
        \end{tabular} &
        \begin{tabular}{c|cc}
          0 \\ 1 & $\varphi_1$ \RKBar $\varphi_1 - 2 \varphi_3$ & $2\varphi_3$
        \end{tabular} &
        \begin{tabular}{c|cccc}
          0 \\ $\frac{3}{4}$ & $\frac{3}{4}\varphi_1\left(\frac{3}{4} \cdot\right)$ \RKBar $\varphi_1 - \frac{32}{9} \varphi_3$ & $\frac{32}{9}\varphi_3$
        \end{tabular} \\
        Exponential Euler & ExpRB32 & ExpRB42 \\
      \end{tabular}
      \caption{\PS{TODO}}\label{tab:exp_rk_butcher}
    \end{table}

    \paragraph{}
    Among exponential Runge--Kutta methods, we looked at the ExpRB32 and ExpRB42 methods \PS{citation + distinction RK Rosenbrock}.
