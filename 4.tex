\chapter{Analysis of a new type of time integration methods}

  \paragraph{}
  In this thesis, we were interested in finding solutions to steady problems.
  As we explained in the previous part, we use implicit time integration methods to get efficiently solutions of such problems.
  This is a pretty standard choice: most computational fluid dynamics solvers use implicit time integration methods to solve steady problems.
  The reason is because they can use larger time steps than their explicit counterparts.
  Because of this advantage, they are also often used when solving unsteady problems but with large time steps.
  Indeed, solving an unsteady problem with large time steps is similar to solving a steady problem.
  Most of what is done towards the steady problem solve can therefore be reused for unsteady problem solves with large time steps.

  \paragraph{}
  Even if implicit time integration methods are quite standard when solving problems with large time steps, there are other less conventional methods we could choose from.
  We could step out from the explicit implicit dichotomy, and decide to use IMplicit-EXplicit, or IMEX, methods.
  They split the function from the ordinary differential equation (\ref{eq:ode}) in two parts: the stiff part than is integrated by an implicit method, because of its stiffness, and the other part that can just be integrated with an explicit method.
  The Additive Semi-Implicit Runce--Kutta methods, or ASIRK methods, are such methods \cite{Zhong1996}.
  They are already in use in computational fluid dynamics problems, such as fluid-structure interaction problems \cite{HuangPerssonZahr2019}.
  Previous work already implemented ASIRK methods in our solver CEDRE for specific multiphysic applications.
  Some methods are even less common and correspond to a total paradigm shift: the parallel time integration methods \cite{Nievergelt1964, LionsMadayTurinici2001}.
  Just as we classically split the computational domain over processes and compute the spatial discretisation method in parallel, parallel time integration methods decompose the time integration interval into subintervals.
  They then solve the ordinary differential equation on each interval concurrently then ensure the continuity between the subintervals.
  They then iterate with Newton's method to find the solution over the whole time integration interval.
  As a result, they can approximate accurately the solution at later time without knowing accurately the solution at previous time.
  Despite being nontraditional, they were successfully used to solve fluid structure interaction problems or Navier--Stokes problems \cite{GanderVandewalle2007}.
  They were even more recently used to solve simple turbulent flow problems \cite{Lunet2018}.
  They can even be used in a more convoluted way, with for instance exponential methods \cite{GanderGuettel2013}.
  Just as parallel time integration is inspired from parallel spatial discretisation, some time integration methods are inspired from spectral discretisation methods.
  Time spectral methods, that were originally used for fluid dynamic time periodic problems \cite{GopinathJameson2005, GopinathJameson2006}, are now used on non-periodic problems \cite{EkiciDjeddiLiEtAl2020}.
  But as both time parallel integration methods and time spectral methods are considered modern methods and highly unconventional, they do not seem appropriate for an industrial solver such as ours.


  \section{Exponential integration methods}

    \paragraph{}
    We decided to look at an other class of time integration methods: exponential methods.
    Despite being known for a long time \cite{Pope1963}, they were not widely used in computational fluid dynamics, because of some difficulties that we will explain later, but still interested some \cite{EdwardsTuckermanFriesnerEtAl1994}.
    They started to come back in the literature \cite{HochbruckOstermann2005, HochbruckOstermann2010} and are now being use on applications similar to ours \cite{NieZhangZhao2006, BhattKhaliqWade2018}.

    \paragraph{}
    We start from an ordinary differential equation
    \begin{equation}\label{eq:ode_2}
      \frac{\mathrm{d} y}{\mathrm{d} t} = f\left(y\right) \ .
    \end{equation}
    This is the same as the ordinary differential equation (\ref{eq:ode}) from the previous part but with different notations.
    The main idea of exponential integration methods is to start from the ordinary differential equation (\ref{eq:ode_2}) and split the function $f$ in a linear part and a nonlinear part:
    \begin{equation}
      \frac{\mathrm{d} y}{\mathrm{d} t} = Ly + N\left(y\right) \ .
    \end{equation}
    This decomposition is sometimes natural for some particular equations, but in the more general case it is always possible: it consists in choosing a linear part $L$ then setting the nonlinear part $N\left(y\right) = f(y) - Ly$.
    There are then an infinite number of decomposition, but we will see later that some are more interesting.

    \paragraph{}
    To define the time integration step, we start from the current estimate of the solution $y_n$ that we assume exact : $y_n = y\left(t_n\right)$.
    We note $\Delta t = t_{n+1} - t_n$ as we work here with a fixed $n$.
    We can integrate equation (\ref{eq:ode_2}) using the variation of constants formula to get:
    \begin{equation}\label{eq:ode_int}
      y\left(t_{n+1}\right) = e^{\Delta t L} y_n + \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} N\left(y\left(t\right)\right) \mathrm{d}t \ .
    \end{equation}
    The exponential integration methods then approximate the integral to compute the next value $y_{n+1}$.
    What defines the method is how it approximates this integral.
    As we can see in equation (\ref{eq:ode_int}), the linear part is treated exactly and the nonlinear one is approximated.
    If there is no nonlinear part, with $N = 0$, then the solution $y_{n+1}$ is exact.
    If there is no linear part, with $L = 0$, then equation (\ref{eq:ode_int}) transforms into
    \begin{equation}\label{eq:ode_int_classic}
      y\left(t_{n+1}\right) = y_n + \int_{t_n}^{t_{n+1}} N\left(y\left(t\right)\right) \mathrm{d}t
    \end{equation}
    and the exponential integration method behaves like a standard time integration method.
    This is the advantage of exponential integration methods: at best they are exact methods, and at worse they are equivalent to traditional methods.

    \paragraph{}
    Similarly to classic methods, there are explicit \cite{BhattKhaliqWade2018} and implicit \cite{NieZhangZhao2006} exponential integration methods.
    It depends on whether it uses $y_{n+1}$ or not to compute the integral from equation (\ref{eq:ode_int}).

    \paragraph{}
    Before continuing, we need to define some functions that are going to be convenient later on.
    We consider the functions $\varphi_k$, defined by:
    \begin{equation}
      \left\{\begin{aligned}
        \varphi_0 &: z \mapsto e^z \\
        \forall k \in \mathbb{N}^*, \varphi_{k} &: z \mapsto \int_0^1 e^{\left(1 - \theta\right)z} \frac{\theta^{k-1}}{\left(k-1\right)!} \mathrm{d}\theta
      \end{aligned}\right.\ .
    \end{equation}
    We could also define them using the recurrence relation:
    \begin{equation}
      \left\{\begin{aligned}
        \varphi_0 &: z \mapsto e^z \\
        \forall k \in \mathbb{N}, \varphi_{k+1} &: z \mapsto \frac{\varphi_k\left(z\right) - \varphi_k\left(0\right)}{z} \ .
      \end{aligned}\right.
    \end{equation}
    Finally, we can also use their analytic formula:
    \begin{equation}
      \forall k \in \mathbb{N}, \varphi_{k} : z \mapsto \sum_{i = 0}^{+\infty} \frac{z^i}{\left(i + k\right)!} \ .
    \end{equation}
    This analytic formula ensure it is well defined for squared matrices.


  \section{Exponential Rosenbrock--Euler method}

    \subsection{Definition of the exponential Rosenbrock--Euler method}

      \paragraph{}
      To better understand exponential integration methods, we take the most basic one: the exponential Euler method.
      Just as the Euler method assumes that $N\left(y\right)$ is constant and equal to $N\left(y_n\right)$ in equation (\ref{eq:ode_int_classic}), its exponential counterpart makes the same assumption but in equation (\ref{eq:ode_int}).
      It then gives:
      \begin{equation}
        y_{n+1} = e^{\Delta t L} y_n + \Delta t \varphi_1\left(\Delta t L\right) N\left(y_n\right)
      \end{equation}
      using the $\varphi_1$ function defined above, and finally:
      \begin{equation}
        y_{n+1} = y_n + \Delta t \varphi_1\left(\Delta t L\right) f\left(y_n\right) \ .
      \end{equation}
      We note the difference with the corresponding standard Euler method, for which the $\varphi$ function is replaced with the identity function.
      If there is no linear part in the decomposition, with $L = 0$, then nothing is treated differently by the exponential method, and we recover the standard explicit Euler method.

      \paragraph{}
      In this method, we take the linearisation $L = f'\left(y_n\right)$.
      Because of this choice, the method is called the exponential Rosenbrock--Euler method.
      We will discuss the nomenclature below.
      This choice feels natural and minimise the error of the method as is shown in the following.
      This choice is frequent among exponential integration methods, despite some methods using a fixed linearisation $L = f'\left(y_0\right)$.
      The difference is that a new Jacobian matrix is required at each iteration of the time integration method, but it is something we are used to deal with our traditional methods.
      It is worth noting that we might want to try the implicit equivalent of this method by taking $N\left(y\right) = N\left(y_{n+1}\right)$ in equation (\ref{eq:ode_int}).
      However, because we took $L = f'\left(y_n\right)$, $N'\left(y\right) = 0$ and after a linearisation the two variants are equivalent.


    \subsection{Analysis of the exponential Rosenbrock--Euler method}

      \paragraph{}
      From the definition on the method and equation (\ref{eq:ode_int}), we have that the error made after one step is:
      \begin{equation}
        \begin{aligned}
          y\left(t_{n+1}\right) - y_{n+1} &= \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} N\left(y\left(t\right)\right) \mathrm{d}t  - \Delta t \varphi_1\left(\Delta t L\right) N\left(y_n\right) \\
          &= \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} \left( N\left(y\left(t\right)\right) - N\left(y_n\right) \right)
        \end{aligned}
      \end{equation}
      We take the Taylor serie of the nonlinearity:
      \begin{equation}
        N\left(y\right) = \sum_{i = 0}^{+\infty} \frac{N^{\left(i\right)}\left(y_n\right)}{i!}\left(y - y_n\right)^i
      \end{equation}
      and in particular, using the fact that
      \begin{equation}
        \begin{aligned}
          y\left(t\right) &= y_n + y'\left(t_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right) \\
          & = y_n + f\left(y_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right)
        \end{aligned}
      \end{equation}
      the partial serie of order 2 is:
      \begin{equation}
        N\left(y\left(t\right)\right) = N\left(y_n\right) + N'\left(y_n\right)f\left(y_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right) \ .
      \end{equation}
      The error of the method is then:
      \begin{equation}
        \begin{aligned}
          y\left(t_{n+1}\right) - y_{n+1} &= \int_{t_n}^{t_{n+1}} e^{\left(t_{n+1} - t\right) L} \left( N'\left(y_n\right)f\left(y_n\right)\left(t - t_n\right) + O\left(\left(t - t_n\right)^2\right) \right) \mathrm{d}t \\
          &= \Delta t^2\varphi_2\left(\Delta t L\right) N'\left(y_n\right)f\left(y_n\right) + O\left(\Delta t^3\right) \ .
        \end{aligned}
      \end{equation}
      We now understand the decomposition chosen earlier: if $L = f'\left(y_n\right)$ and therefore $N'\left(y_n\right) = 0$ then $y\left(t_{n+1}\right) - y_{n+1} = O\left(\Delta t^3\right)$ and the method is of order 2.
      This is a noticeable property of this method: despite being a single step method it has a second order of accuracy.
      A more rigorous analysis can be found in \cite{HochbruckOstermannSchweitzer2009}.

      \paragraph{}
      It is much harder to analyse the stability of this method, and more generally of exponential methods.
      Indeed, the stability analysis is done on the Dahlquist test equation (\ref{eq:dahlquist}).
      But as this equation is linear, exponential methods can solve them exactly no matter the time step size.
      We can still write the single step equation (\ref{eq:single_step}) $y_{n+1} = g\left(\Delta tJ\right)y_n$ with $g\left(z\right) = e^z$, and get the corresponding stability region which is the left half complex plane.
      We could then conclude that the method is A-stable.
      The issue with exponential methods is that the standard stability analysis is no longer pertinent.
      We tried to do a better stability analysis for simple exponential methods such as the exponential Rosenbrock--Euler method, but we did not succeed.
      Some work in the literature do define some stability notions for exponential methods \cite{DuZhu2004}, but usually take a linear $N$.
      It goes against the idea that all the linear part of $f$ is in $L$ and $N$ is purely nonlinear.
      We did not find a stability analysis that was satisfying to us.
      We will probably find such analysis, today of maybe in the future, in research from a more mathematical context than the one we are in, so we did not insist on finding such an analysis.


  \section{Exponential Runge--Kutta and Rosenbrock methods}

    \paragraph{}
    When we introduced explicit methods, we used more convoluted approximations than the explicit Euler methods for the integral in equation (\ref{eq:ode_int_classic}).
    This gave the Runge--Kutta methods.
    We can do the same for the integral in equation (\ref{eq:ode_int}) to get exponential Runge--Kutta methods.
    The difference with a standard Runge--Kutta method is that the quadrature coefficients are now functions of the matrix $L$:
    \begin{equation}
      \left\{\begin{aligned}
        y_{n+1} &= e^{\Delta t L} y_n + \Delta t \sum_{i = 1}^k b_i\left(\Delta t L\right) N\left(y_{n,i}\right) \\
        \textrm{with}\quad y_{n,i} &= e^{c_i \Delta t L} y_n + \Delta t \sum_{j = 1}^{i-1} a_{ij}\left(\Delta t L\right) N\left(y_{n,j}\right)
      \end{aligned}\right. \ .
    \end{equation}
    The Butcher tableau that we used to represent the Runge--Kutta methods is also used to represent exponential Runge--Kutta methods.
    For instance, the Butcher tableau of the exponential Rosenbrock--Euler method, which is a special simple case of exponential Runge--Kutta methods, is shown in table \ref{tab:exp_rb_butcher}.

    \paragraph{}
    Until now, we have not been exact when naming the methods we constructed here.
    From the literature, exponential Runge--Kutta methods are defined just as we did above, except they use a fixed linearisation: $L = f'\left(y_0\right)$ \cite{HochbruckOstermann2005}.
    It is because they were developped for semilinear parabolic problems:
    \begin{equation}
      \frac{\mathrm{d} y}{\mathrm{d} t} + Ay = g\left(y\right) \ .
    \end{equation}
    They are used when the remainder $r$ is small or at least bounded in terms of $A$.
    This is not the case on our applications, and on many others so that is why another type of exponential methods were introduced: exponential Rosenbrock methods \cite{HochbruckOstermannSchweitzer2006}.
    They can be seen as a variation of the exponential Runge--Kutta methods where the linearisation is done at each time step.
    The issue is that a new Jacobian matrix is needed at each iteration, but once again we are used to this with implicit methods.

    \paragraph{}
    The nomenclature is a bit blurry to us, as the distinction between exponential Runge--Kutta and exponential Rosenbrock methods is not the same than the one between Runge--Kutta methods and Rosenbrock methods.
    First of all, the exponential Rosenbrock method is not implicit contrary to its standard counterpart.
    Furthermore, it would make sense to us to name an exponential integration method base on the underlying method used to approximate the integral from equation (\ref{eq:ode_int}), and exponential Rosenbrock methods do not use Rosenbrock methods to do it.
    The only parallel we found was that Rosenbrock methods do a linearisation at each inner stage, and the exponential Rosenbrock methods do a linearisation at each step of the time integration.
    It just seems that in the literature, the term \emph{Rosenbrock} indicates that the linearisation is computed at each step instead of once at the beginning, with no apparent link with Rosenbrock methods.

    \paragraph{}
    Just as the Runge--Kutta methods has order conditions for its quadrature coefficients, the exponential Rosenbrock methods quadrature functions must follow some rules to ensure the expected order.
    To be consistant, it must have:
    \begin{equation}
      \sum_{i = 1}^k b_i = \varphi_1
    \end{equation}
    and it must also have
    \begin{equation}
       \sum_{j = 1}^{i-1} a_{ij} = z \mapsto c_i \varphi_1\left(c_i z\right), \quad 1 \leq i \leq k
    \end{equation}
    to achieve a second order.
    As we want to use methods with multiple stages, it is reasonable to want them to have an higher order than the single stage exponential Euler method.
    Using this two conditions, exponential Rosenbrock methods can be written in the form:
    \begin{equation}
      \left\{\begin{aligned}
        y_{n+1} &= y_n  + \Delta t \varphi_1\left(\Delta t L\right) f\left(y_n\right) + \Delta t \sum_{i = 1}^k b_i\left(\Delta t L\right) D_{n,i} \\
        \textrm{with}\quad y_{n,i} &= y_n  + c_i \Delta t \varphi_1\left(c_i \Delta t L\right) f\left(y_n\right) + \Delta t \sum_{j = 1}^{i-1} a_{ij}\left(\Delta t L\right) D_{n,j}
      \end{aligned}\right.
    \end{equation}
    using $D_{n,i} = N\left(y_{n,i}\right) - N\left(y_n\right)$.
    This form shows that exponential Rosenbrock methods are variations to the exponential Euler method.
    The defects $D_{n,i}$ are small in size, so less computational effort can be spend when computing their contribution.

    \paragraph{}
    Among exponential Rosenbrock methods, we looked at the ExpRB32 method from \cite{HochbruckOstermannSchweitzer2009} and ExpRB42 methods from \cite{Luan2017}.
    They were developed as adaptive methods, but we will use them without the error estimate as standard methods.
    They are both 2-stage exponential Rosenbrock methods, and achieve respectively a third and fourth order of accuracy.
    This shows the quality of exponential Rosenbrock methods.
    Their Butcher tableau are in table \ref{tab:exp_rb_butcher}.

    \begin{table}
      \begin{tabular}{M{.25\textwidth}M{.3\textwidth}M{.3\textwidth}c}
        \begin{tabular}{c|c}
          \multicolumn{1}{c}{} \\ 0 \RKBar $\varphi_1$
        \end{tabular} &
        \begin{tabular}{c|cc}
          0 \\ 1 & $\varphi_1$ \RKBar $\varphi_1 - 2 \varphi_3$ & $2\varphi_3$
        \end{tabular} &
        \begin{tabular}{c|cccc}
          0 \\ $\frac{3}{4}$ & $\frac{3}{4}\varphi_1\left(\frac{3}{4} \cdot\right)$ \RKBar $\varphi_1 - \frac{32}{9} \varphi_3$ & $\frac{32}{9}\varphi_3$
        \end{tabular} & \\[20pt]
        Exponential Rosenbrock--Euler & ExpRB32 & ExpRB42 \\
      \end{tabular}
      \caption{Butcher tableau for some exponential Rosenbrock integration methods}\label{tab:exp_rb_butcher}
    \end{table}
